{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Polish companies bankruptcy dataset: Polish.mat\n",
    "\n",
    "Polish.mat (16.6MB) includes five-year data\n",
    "(1:7027, 2:10173, 3:10503, 4:9792, 5:5910)\n",
    "\n",
    "the last column in each year is lable includes 2 classes\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'year1', 'year2', 'year3', 'year4', 'year5'])\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = sio.loadmat('./Polish.mat')\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.37951</td>\n",
       "      <td>0.396410</td>\n",
       "      <td>2.04720</td>\n",
       "      <td>32.3510</td>\n",
       "      <td>0.38825</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.33050</td>\n",
       "      <td>1.13890</td>\n",
       "      <td>0.504940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.397180</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.658</td>\n",
       "      <td>4.4158</td>\n",
       "      <td>7.42770</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.472250</td>\n",
       "      <td>1.94470</td>\n",
       "      <td>14.7860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.99601</td>\n",
       "      <td>1.69960</td>\n",
       "      <td>0.497880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.420020</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.350</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>60.98700</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.69592</td>\n",
       "      <td>0.267130</td>\n",
       "      <td>1.55480</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.43695</td>\n",
       "      <td>1.30900</td>\n",
       "      <td>0.304080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.817740</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.694840</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.270</td>\n",
       "      <td>2.7185</td>\n",
       "      <td>5.20780</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.458790</td>\n",
       "      <td>2.49280</td>\n",
       "      <td>51.9520</td>\n",
       "      <td>0.14988</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.86610</td>\n",
       "      <td>1.05710</td>\n",
       "      <td>0.573530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.142070</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.435</td>\n",
       "      <td>4.2228</td>\n",
       "      <td>5.54970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.61323</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>1.40630</td>\n",
       "      <td>-7.3128</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.63070</td>\n",
       "      <td>1.15590</td>\n",
       "      <td>0.386770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.484310</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.210</td>\n",
       "      <td>2.8692</td>\n",
       "      <td>7.89800</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>0.012898</td>\n",
       "      <td>0.70621</td>\n",
       "      <td>0.038857</td>\n",
       "      <td>1.17220</td>\n",
       "      <td>-18.9070</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>0.41600</td>\n",
       "      <td>1.67680</td>\n",
       "      <td>0.293790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020169</td>\n",
       "      <td>0.043904</td>\n",
       "      <td>1.01220</td>\n",
       "      <td>1.259400</td>\n",
       "      <td>13.4720</td>\n",
       "      <td>12.4320</td>\n",
       "      <td>49.117</td>\n",
       "      <td>7.4313</td>\n",
       "      <td>2.27990</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>-0.578050</td>\n",
       "      <td>0.96702</td>\n",
       "      <td>-0.800850</td>\n",
       "      <td>0.16576</td>\n",
       "      <td>-67.3650</td>\n",
       "      <td>-0.57805</td>\n",
       "      <td>-0.578050</td>\n",
       "      <td>-0.40334</td>\n",
       "      <td>0.93979</td>\n",
       "      <td>-0.390040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064073</td>\n",
       "      <td>1.482000</td>\n",
       "      <td>1.06410</td>\n",
       "      <td>-0.018084</td>\n",
       "      <td>110.7200</td>\n",
       "      <td>44.7590</td>\n",
       "      <td>81.220</td>\n",
       "      <td>4.4940</td>\n",
       "      <td>5.13050</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>-0.179050</td>\n",
       "      <td>1.25530</td>\n",
       "      <td>-0.275990</td>\n",
       "      <td>0.74554</td>\n",
       "      <td>-120.4400</td>\n",
       "      <td>-0.17905</td>\n",
       "      <td>-0.154930</td>\n",
       "      <td>-0.26018</td>\n",
       "      <td>1.17490</td>\n",
       "      <td>-0.326590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148880</td>\n",
       "      <td>0.548240</td>\n",
       "      <td>0.85112</td>\n",
       "      <td>-0.522430</td>\n",
       "      <td>9.8526</td>\n",
       "      <td>3.4892</td>\n",
       "      <td>207.870</td>\n",
       "      <td>1.7559</td>\n",
       "      <td>9.95270</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>-0.108860</td>\n",
       "      <td>0.74394</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>1.08780</td>\n",
       "      <td>-17.0030</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.109180</td>\n",
       "      <td>0.12531</td>\n",
       "      <td>0.84516</td>\n",
       "      <td>0.093224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183200</td>\n",
       "      <td>-1.167700</td>\n",
       "      <td>1.18320</td>\n",
       "      <td>6.092400</td>\n",
       "      <td>13.8860</td>\n",
       "      <td>6.0769</td>\n",
       "      <td>83.122</td>\n",
       "      <td>4.3911</td>\n",
       "      <td>0.95575</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>-0.105370</td>\n",
       "      <td>0.53629</td>\n",
       "      <td>-0.045578</td>\n",
       "      <td>0.91478</td>\n",
       "      <td>-56.0680</td>\n",
       "      <td>-0.10537</td>\n",
       "      <td>-0.109940</td>\n",
       "      <td>0.86460</td>\n",
       "      <td>0.95040</td>\n",
       "      <td>0.463670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.227250</td>\n",
       "      <td>1.05220</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>7.7332</td>\n",
       "      <td>4.7174</td>\n",
       "      <td>136.850</td>\n",
       "      <td>2.6672</td>\n",
       "      <td>2.79270</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43406 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0        1         2        3         4        5         6   \\\n",
       "0     0.200550  0.37951  0.396410  2.04720   32.3510  0.38825  0.249760   \n",
       "1     0.209120  0.49988  0.472250  1.94470   14.7860  0.00000  0.258340   \n",
       "2     0.248660  0.69592  0.267130  1.55480   -1.1523  0.00000  0.309060   \n",
       "3     0.081483  0.30734  0.458790  2.49280   51.9520  0.14988  0.092704   \n",
       "4     0.187320  0.61323  0.229600  1.40630   -7.3128  0.18732  0.187320   \n",
       "...        ...      ...       ...      ...       ...      ...       ...   \n",
       "5905  0.012898  0.70621  0.038857  1.17220  -18.9070  0.00000  0.013981   \n",
       "5906 -0.578050  0.96702 -0.800850  0.16576  -67.3650 -0.57805 -0.578050   \n",
       "5907 -0.179050  1.25530 -0.275990  0.74554 -120.4400 -0.17905 -0.154930   \n",
       "5908 -0.108860  0.74394  0.015449  1.08780  -17.0030 -0.10886 -0.109180   \n",
       "5909 -0.105370  0.53629 -0.045578  0.91478  -56.0680 -0.10537 -0.109940   \n",
       "\n",
       "           7        8         9   ...        55        56       57        58  \\\n",
       "0     1.33050  1.13890  0.504940  ...  0.121960  0.397180  0.87804  0.001924   \n",
       "1     0.99601  1.69960  0.497880  ...  0.121300  0.420020  0.85300  0.000000   \n",
       "2     0.43695  1.30900  0.304080  ...  0.241140  0.817740  0.76599  0.694840   \n",
       "3     1.86610  1.05710  0.573530  ...  0.054015  0.142070  0.94598  0.000000   \n",
       "4     0.63070  1.15590  0.386770  ...  0.134850  0.484310  0.86515  0.124440   \n",
       "...       ...      ...       ...  ...       ...       ...      ...       ...   \n",
       "5905  0.41600  1.67680  0.293790  ...  0.020169  0.043904  1.01220  1.259400   \n",
       "5906 -0.40334  0.93979 -0.390040  ... -0.064073  1.482000  1.06410 -0.018084   \n",
       "5907 -0.26018  1.17490 -0.326590  ...  0.148880  0.548240  0.85112 -0.522430   \n",
       "5908  0.12531  0.84516  0.093224  ... -0.183200 -1.167700  1.18320  6.092400   \n",
       "5909  0.86460  0.95040  0.463670  ... -0.052186 -0.227250  1.05220  0.003196   \n",
       "\n",
       "            59       60       61      62        63   64  \n",
       "0       8.4160   5.1372   82.658  4.4158   7.42770  0.0  \n",
       "1       4.1486   3.2732  107.350  3.4000  60.98700  0.0  \n",
       "2       4.9909   3.9510  134.270  2.7185   5.20780  0.0  \n",
       "3       4.5746   3.6147   86.435  4.2228   5.54970  0.0  \n",
       "4       6.3985   4.3158  127.210  2.8692   7.89800  0.0  \n",
       "...        ...      ...      ...     ...       ...  ...  \n",
       "5905   13.4720  12.4320   49.117  7.4313   2.27990  1.0  \n",
       "5906  110.7200  44.7590   81.220  4.4940   5.13050  1.0  \n",
       "5907    9.8526   3.4892  207.870  1.7559   9.95270  1.0  \n",
       "5908   13.8860   6.0769   83.122  4.3911   0.95575  1.0  \n",
       "5909    7.7332   4.7174  136.850  2.6672   2.79270  1.0  \n",
       "\n",
       "[43406 rows x 65 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Five years of data are stitched together.\n",
    "import pandas as pd\n",
    "\n",
    "data1 = pd.DataFrame(data['year1'])\n",
    "data2 = pd.DataFrame(data['year2'])\n",
    "data3 = pd.DataFrame(data['year3'])\n",
    "data4 = pd.DataFrame(data['year4'])\n",
    "data5 = pd.DataFrame(data['year5'])\n",
    "\n",
    "data = pd.concat([data1,data2,data3,data4,data5],axis=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(64,axis=1)\n",
    "y = data.loc[:,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_pca.shape (43406, 32)\n"
     ]
    }
   ],
   "source": [
    "# Use PCA to reduce to 32 dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 32\n",
    "X_pca = PCA(n_components = n_components).fit_transform(X)\n",
    "\n",
    "print('X_pca.shape',X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.99520045e+03, -1.85822213e+03, -1.17773884e+03, ...,\n",
       "         8.31336829e-01, -7.29895302e-01,  1.18911841e-01],\n",
       "       [-1.73140196e+03, -1.07121400e+03, -1.31502042e+03, ...,\n",
       "         1.86285670e+00, -1.06295402e+00,  1.94640721e+00],\n",
       "       [-1.75085467e+03, -1.02653893e+03, -1.19538184e+03, ...,\n",
       "         1.20239497e+00, -8.37715065e-01, -5.74309391e-01],\n",
       "       ...,\n",
       "       [-1.69285296e+03, -9.01748784e+02, -5.33878840e+03, ...,\n",
       "         1.12681399e+00, -8.87994342e-01, -1.14149871e+00],\n",
       "       [-1.85321414e+03, -1.23383773e+03, -7.33778650e+03, ...,\n",
       "        -4.61951707e-02, -1.33236241e+00, -6.08946060e-01],\n",
       "       [-1.78430197e+03, -1.11564450e+03, -4.04561852e+03, ...,\n",
       "         1.01831657e+00, -9.85229331e-01, -1.87137936e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    41315\n",
       "1.0     2091\n",
       "Name: 64, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The category is completely unbalanced\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the test set and training set, because the data is unbalanced, so add hierarchical sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_pca_train,x_pca_test,y_train,y_test = train_test_split(X_pca,y,test_size=0.2,random_state=11,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# One-Hot\n",
    "import keras\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               16896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 280,578\n",
      "Trainable params: 280,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  Build a full neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(32,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34724 samples, validate on 8682 samples\n",
      "Epoch 1/10\n",
      "34724/34724 [==============================] - 6s 187us/step - loss: 81.9768 - accuracy: 0.9206 - val_loss: 5.8544 - val_accuracy: 0.9408\n",
      "Epoch 2/10\n",
      "34724/34724 [==============================] - 5s 149us/step - loss: 5.1679 - accuracy: 0.9391 - val_loss: 1.5190 - val_accuracy: 0.9512\n",
      "Epoch 3/10\n",
      "34724/34724 [==============================] - 5s 153us/step - loss: 3.7977 - accuracy: 0.9421 - val_loss: 1.4070 - val_accuracy: 0.9425\n",
      "Epoch 4/10\n",
      "34724/34724 [==============================] - 6s 166us/step - loss: 3.6256 - accuracy: 0.9404 - val_loss: 1.0736 - val_accuracy: 0.9468\n",
      "Epoch 5/10\n",
      "34724/34724 [==============================] - 5s 145us/step - loss: 2.3858 - accuracy: 0.9412 - val_loss: 1.3360 - val_accuracy: 0.9517\n",
      "Epoch 6/10\n",
      "34724/34724 [==============================] - 5s 154us/step - loss: 2.3386 - accuracy: 0.9401 - val_loss: 0.7186 - val_accuracy: 0.9492\n",
      "Epoch 7/10\n",
      "34724/34724 [==============================] - 6s 161us/step - loss: 2.7542 - accuracy: 0.9396 - val_loss: 1.5007 - val_accuracy: 0.9499\n",
      "Epoch 8/10\n",
      "34724/34724 [==============================] - 5s 146us/step - loss: 3.1823 - accuracy: 0.9392 - val_loss: 1.3121 - val_accuracy: 0.9516\n",
      "Epoch 9/10\n",
      "34724/34724 [==============================] - 5s 146us/step - loss: 3.1832 - accuracy: 0.9322 - val_loss: 1.9121 - val_accuracy: 0.9319\n",
      "Epoch 10/10\n",
      "34724/34724 [==============================] - 5s 158us/step - loss: 2.5699 - accuracy: 0.9282 - val_loss: 4.9154 - val_accuracy: 0.9331\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_pca_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_pca_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.9154141606465656\n",
      "Test accuracy: 0.9330799579620361\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_pca_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
